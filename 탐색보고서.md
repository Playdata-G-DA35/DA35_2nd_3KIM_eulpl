## 크롤링 사이트

1. 사이트 선정 과정
    - 처음엔 특정 지역을 해쉬태그로한 인스타그램 포스트를 크롤링 하려 했다. 하지만 인스타그램 포스트에서는 정확한 위치 정보가 아닌 인근으로 통합된 태그만 얻을 수 있었고 (ex 을지로 3가) 가게의 위치를 알아내는데 문제가 있기 때문에, 정확한 위치가 표기된 게시물이 많은 네이버 블로그 포스트를 대상으로 크롤링을 진행하게 되었다.

2. 분석 정보
    - 먼저 블로그 게시물을 검색하는 과정에서, 페이지 url에서 검색 시작 날짜와 끝 날짜를 지정할 수 있기 때문에 게시물이 작성된 날짜를 필터링할 수 있었다.
    - 검색어에서 해쉬태그를 통해 관련된 게시물을 수집할 수 있었는데, 지역명으로만 검색했을 경우에는 원하는 정보인 맛집이외의 장소들이 링크되어 있는 경우도 많았기 때문에, 검색어는 #장소 + #맛집 형태로 변경했다.
    - #장소 + #맛집 형태로 검색했을 경우, 거의 대부분의 게시물이 소개하는 가게의 위치정보를 네이버 지도 링크를 통해 제공하기 때문에 Selenium을 활용하여 해당 가게의 이름과 주소를 수집할 수 있었다.


### 각 지역구의 맛집 크롤링
    - 네이버 블로그에서 합정, 홍대, 연남 지역구들을 각 태그별로 검색
      - 동일한 가게이름 count
      - 블로그 안의 iframe 내 가게 이름, 주소 크롤링
        -  Search_naverblog.png
        -  inner_blog.png
  ### 어려웠던 점
    -  css selector를 사용해 링크를 선택하려 했으나 가게 이름과 위치 정보를 타겟으로 했을 때 selector를 변경해봐도 계속해서 'no such element' 오류가 발생했다. 네이버 지도 링크는 다른 웹페이지가 삽입된 형태이기 때문에 웹드라이버를 iframe으로 변환시킨뒤에 동일한 작업을 함으로써 해결했다.
## 프로젝트 구현 방식
 - selenium : 브라우저를 열어서, url 및 page별 title 정보 및 가게 이름, 주소 추출
 - iframe : 웹페이지 내의 다른 웹페이지를 삽입할 수 있게 하는 요소 (동적으로 웹페이지를 로드하거나 특정 영역에만 웹페이지를 표시) ==> css selector를 사용해 iframe 요소 찾는 함수 사용
 - css selector : 웹 페이지별 필요한 정보 태그 추적
 - 예외처리 (exception) : 페이지 내의 가게의 이름이나 주소가 없으면 예외처리
 - 반복문 및 조건문 (for, if) : 반복 추출 시에 정보가 중복되면 count+1 
 - pymysql : DB 연동 ==> 테이블 생성, 데이터 삽입
 - pandas : 데이터프레임을 이용해 csv파일 생성
  
  

# 데이터 셋
  - 크롤링한 데이터
    - march_week4_Hapjung.csv
    - march_week4_Hongdae.csv
    - march_week4_Yunnam.csv
  - DB의 테이블 형태
    - Table_Hapjung.png